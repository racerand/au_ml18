{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Text Classification using Sklearn\n", "For those of you that completed the coding exercise in week 1 this will look familiar and it should therefore be quite easy. In this exercise we solve the same problem using Sklearn.\n", "Try to do it anyways as it may give a wider perspective on the problem.\n", "\n", "In this exercise we try to use machine learning for a problem where the featuere data is strings instead of vectors of numbers (like it would be for spam detection).\n", "\n", "The simple trick to continue as before is to turn strings into vectors of numbers and then attack the problem as in the last exercise.\n", "In this exercise we use the simplest way of turning strings into vectors and then applying a standard learning algorithm for classifying text.\n", "\n", "##  The Problem\n", "In this exercise we will try to help the Danish Business Authority (DBA). Each company in Denmark is registered at DBA with a <i>purpose statement</i> and an <i>industry code</i>. For example:  \n", "<table>\n", " <tr>\n", "  <td><b>Purpose Statement</b></td>\n", "  <td><b>Industry Code</b></td>\n", " </tr>\n", "<tr>\n", "  <td>Make sushi to people</td>\n", "  <td>1113</td>\n", "</tr>\n", "<tr>\n", "  <td>Make spaghetti to people</td>\n", "  <td>1113</td>\n", "</tr>\n", "<tr>\n", "  <td>Fix peoples laptop</td>\n", "  <td>324</td>\n", "</tr>\n", "<tr>\n", "  <td>Make lasagna to people</td>\n", "  <td>1113</td>\n", "</tr>\n", "<tr>\n", "  <td>Fix peoples smartphones</td>\n", "  <td>324</td>\n", "</tr>\n", "</table>\n", "\n", "In the above example is seems that the industry code 1113 has something to do with making food for people, it might be the industry code for restaurants. Similarly 324 might be the company code for technology related business. The above example is made up. You will be working with real data.  \n", "\n", "The DBA suspect some companies provide incorrect industry codes. They want to detect industry codes that are probably incorrect and help new companies select the right industry code. To help with this we want an algorithm that\n", "\n", "    Given a string (the purpose statement of a company) outputs the most likely associated industry code. \n", "\n", "If we are succesful in doing so, we would be able to \n", "    1. suggest industry codes to new companies \n", "    2. find industry codes that are not the most likely (they might be errors) \n", "In the real world there are around <b>650</b> different industry codes. To make the exercise easier, we will only consider two different industry codes: *Restaurants* and *Computer Programming*.\n", "\n", "To see examples of the actual data run the cell below (you do not need to understand the code).\n", "\n"]}, {"cell_type": "code", "execution_count": 1, "metadata": {"scrolled": false}, "outputs": [], "source": ["# first some magic functions you can ignore\n", "%matplotlib inline\n", "%load_ext autoreload\n", "%autoreload 2\n", "\n", "from IPython.display import display    \n", "import pandas as pd\n", "pd.set_option('max_colwidth', 120)\n", "import numpy as np\n", "import os\n", "import urllib\n", "from sklearn.model_selection import train_test_split\n", "\n", "def load_branche_data(keys):\n", "    \"\"\"\n", "    Load the data in branche_data.npz and save it in lists of\n", "    strings and labels (whose entries are in {0,1,..,num_classes-1})\n", "    \"\"\"\n", "    filename = 'branchekoder_formal.gzip'\n", "    if not os.path.exists(filename):\n", "        with open(filename, 'wb') as fh:\n", "            path = \"http://users-cs.au.dk/jallan/ml/data/{0}\".format(filename)\n", "            fh.write(urllib.request.urlopen(path).read())\n", "    data = pd.read_csv(filename, compression='gzip')\n", "    actual_class_names = []\n", "    features = []\n", "    labels = []\n", "    for i, kv in enumerate(keys):\n", "        key = kv[0]\n", "        name = kv[1]\n", "        strings = data[data.branchekode == key].formal.values\n", "        features.extend(list(strings))\n", "        label = [i] * len(strings)\n", "        labels.extend(label)\n", "        actual_class_names.append(name)\n", "    assert len(features) == len(labels)\n", "    features = np.array(features)\n", "    labels = np.array(labels)\n", "    return features, labels, actual_class_names\n", "\n", "\n", "def get_branche_data(keys):\n", "    features, labels, actual_class_names = load_branche_data(keys)\n", "    X_train, X_test, y_train, y_test = train_test_split(features, labels)\n", "    return X_train, X_test, y_train, y_test, actual_class_names\n", "\n", "\n", "def print_errors(classifier, feat, raw_feat, labels, class_names, top=10):\n", "    \"\"\" Print first top errors made \"\"\"\n", "    pred = classifier.predict(feat)\n", "    idx = pred != labels\n", "    top_errors = np.nonzero(idx)\n", "    top_errors = top_errors[0]\n", "    top_errors = top_errors[0:top]\n", "    print('*'*30)\n", "    for err_idx in top_errors:        \n", "        print('\\nMispredicted: ', raw_feat[err_idx])\n", "        print('Classifier Prediction: ', pred[err_idx], class_names[pred[err_idx]])\n", "        print('Actual Label: ', labels[err_idx], class_names[labels[err_idx]])\n", "    print('*'*30)\n", "\n", "\n", "keys = [(561010, 'Restauranter'), (620100, 'Computerprogrammering')]\n", "feat_train, feat_test, lab_train, lab_test, cnames = get_branche_data(keys)\n", "class_names = [keys[0][1], keys[1][1]]\n", "print('Lets see some examples')\n", "names = np.array([class_names[i] for i in lab_train])\n", "data = pd.DataFrame(np.c_[names, feat_train], columns=['classname', 'description'])\n", "display(data.head(20))\n", "\n", "rest = data[data.classname == class_names[0]] # notice the cool filtering going on here.\n", "rest_vc = rest.description.value_counts().to_frame()\n", "print(\"\\n\\nThe 20 most frequent purpose statements of Restaurants:\")\n", "display(rest_vc[0:20])\n", "print('-'*80)\n", "\n", "cpu = data[data.classname == class_names[1]]\n", "cpu_vc = cpu.description.value_counts().to_frame() \n", "print(\"\\n\\nThe 20 most frequent purpose statements of Computer Programming\")\n", "display(cpu_vc[0:20])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Text Classification Approach\n", "The first learning algorithm we will use is named Multinomial Naive Bayes (which you may have implemented from scratch last week - here we tools available in Scikit-learn).\n", "\n", "However, like many other learning algorithms this algorithm expects the input to be vectors of numbers, not strings. So, first we need to make strings into vectors of numbers before we can apply the learning algorithm.\n", "Of course the way we transform strings to vectors of numbers is important, here we will try the simplest one that works well enough. \n", "\n", "This is roughly done as follows:\n", "\n", "* First we make a dictionary of words we care about (could be all words in the input) and order the words arbitrarily.\n", "* We then consider a string as a list of unordered words by mapping a string to to the vector v, such that $v[i]$ is the number of times a string contains the i'th word in the dictionary of words.\n", "\n", "This is called a Bag of Words representation (sounds reasonable right).\n", "\n", "After the transformation, we have data on vector form as our supervised learning algorithms like.\n", "\n", "We now turn to Multinomial Naive Bayes. This learning algorithm is simply based on for each class counting the occurrences of words in the associated input vectors (representing strings as bags of words) and use these frequencies to determine which class a new string most likely originates from.\n", "At a high level, to classify a new string, pick the class where the words it contains are most frequent.\n", "\n", "See coding exercise for week 1 for a longer explanation.\n", "\n", "To make strings to vectors we use the sklearn CountVectorizer.\n", "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n", "It has a fit method and a transform method. \n", "* The **fit** makes the dictionary of words and the word mapping, i.e. from word to index in dictionary.\n", "* The **Transform**, takes a vector of strings and makes a vector of vectors (matrix) represented as a sparse matrix/vectors since each string has only few of all the words in the dictionary.\n", "\n", "This vector representation can then be plugged directly into standard learning algorithms which in this case will be Multinomial Naive Bayes.\n", "http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n", "Logistic Regression, Decision Trees, Neural Nets and other learning algorithms could easily be applied as well.\n", "\n", "So Lets first take a look at how to make strings to vectors using the Countvectorizer before we build a classifier.\n", "## The CountVectorizer\n", "In the cell bellow i have shown a simple application of the count vectorizer and how it makes strings to vectors.\n"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["from sklearn.feature_extraction.text import CountVectorizer\n", "import numpy as np\n", "c = CountVectorizer()\n", "strings = ['basic text classification seems basic enough', \n", "           'yes basic text classification is pretty pretty pretty', \n", "           'basic is a pretty start it is']\n", "c.fit(strings)\n", "print('Lets see the dictionary of words')\n", "print(c.get_feature_names())\n", "print('\\nLets the the vectors:\\n')\n", "vectors = c.transform(strings).todense() # otherwise stored in sparse format\n", "z = pd.DataFrame(vectors, index=strings, columns=c.get_feature_names())\n", "display(z)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Exercise 1: Implement a text classification algorithm.\n", "Your job is simply to complete the implementation of naive_bayes_text_classification below by following the specified steps.\n", "\n", "Test by running the cell which runs the test written last. You should get around 95 percent accuracy both on training and test set."]}, {"cell_type": "code", "execution_count": 3, "metadata": {"scrolled": false}, "outputs": [], "source": ["from sklearn.naive_bayes import MultinomialNB\n", "\n", "\n", "def naive_bayes_text_classification(feat_train, feat_test, lab_train, lab_test):\n", "    \"\"\" \n", "        \n", "    Step 1. Vectorize string to bag of words vectors (CountVectorizer)\n", "    Step 2. Create Classifier Object (MultinomialNB)\n", "    Step 3. Fit Classifier to vectorized data and labels (feat_train, feat_test)\n", "    Step 4. Test the insample error using the score function for the classifier - print the score to screen\n", "    Step 5. Test the error on the test data same was as the trainining data - print the score to screen\n", "    Step 6. Return classifier and countvectorizer\n", "    accuracy scores should be around 0.95\n", "    \"\"\"\n", "    \n", "    ### YOUR CODE HERE \n", "    ### END CODE\n", "    \n", "    return classifier, c\n", "\n", "keys = [(561010, 'Restauranter'), (620100, 'Computerprogrammering')]\n", "feat_train, feat_test, lab_train, lab_test, cnames = get_branche_data(keys)\n", "classifier, vectorizer = naive_bayes_text_classification(feat_train, feat_test, lab_train, lab_test)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Lets Analyze the Classifier and the Errors it makes\n", "The algorithm seems to do pretty well. 95% accuracy seem pretty good, but if we must do better we must know where we fail.\n", "We will analyze this in the next cell.\n", "\n", "* First lets see what the classifier is based on i.e. lets see some word counts.\n", "* Next we print some errors and see if we can come up with some reasone why our classifier may fail on this.\n", "\n", "I have written the code, all you need is to look at the output.\n", "Notice that the results you see are random so you can try and run the experiment several times to get more interesting errors perhaps.\n", "\n", "## First lets see some word statistics\n", "When looking at the word counts it seems the classifier should have a chance i think. What do you think? What problems could there be?"]}, {"cell_type": "code", "execution_count": 4, "metadata": {"scrolled": false}, "outputs": [], "source": ["print('Lets see what our classifer learned and bases predictions on')\n", "names = np.array(vectorizer.get_feature_names())\n", "counts = classifier.feature_count_\n", "rest_count = counts[0, :]\n", "prog_count = counts[1, :]\n", "df = pd.DataFrame({'word': names, 'Restauranter': rest_count, 'Computerprogrammering': prog_count})\n", "print('*'*30)\n", "print('Lets see some word counts!')\n", "display(df.sample(20))\n", "print('*'*30)\n", "print('Lets check words with computer in it')\n", "display(df[df.word.str.contains('computer')])\n", "  "]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Lets see the errors then\n", "Again, the code is already there. You just need to consider the result and maybe try to figure out why these examples fail."]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["print('Lets see some of the errors we made, made you can guess why our algorithm fails')\n", "bag_of_words_feat_test = vectorizer.transform(feat_test)\n", "print_errors(classifier, bag_of_words_feat_test, feat_test, lab_test, [x[1] for x in keys], top=10) "]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Exercise 2: Repeat with Decision Trees to see what Decision Trees learn.\n", "### Complete decision_tree_text_classification method below\n", "Essentially replace naive bayes with trees.\n", "Test by running the cell. \n", "Inspect the visualized tree below. Does it make sense to you? **(Make sure you set a max_depth for the tree classifier)**\n", "\n", "What happens with the in sample error when you increase depth?. What happens to the out of sample error (test error)?\n", "Does that seem reasonable?\n"]}, {"cell_type": "code", "execution_count": 6, "metadata": {"scrolled": false}, "outputs": [], "source": ["from sklearn.tree import DecisionTreeClassifier\n", "from sklearn.externals.six import StringIO  \n", "from IPython.display import Image, display\n", "from sklearn.tree import export_graphviz\n", "import pydotplus\n", "\n", "def plot_tree(dtree, feature_names):\n", "    dot_data = StringIO()\n", "    export_graphviz(dtree, out_file=dot_data,\n", "                    filled=True, rounded=True,\n", "                    special_characters=True, feature_names=feature_names)\n", "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n", "    img = Image(graph.create_png())\n", "    display(img)\n", "\n", "def decision_tree_text_classification(feat_train, feat_test, lab_train, lab_test):\n", "    \"\"\"     \n", "    Step 1. Vectorize string to bag of words vectors (CountVectorizer)\n", "    Step 2. Create Classifier Object (DecisionTreeClassifier) - Set max_height (and maybe min_samples_split) in the constructor to control the size of the tree created i.e. min_samples_split=\n", "            if you do not the tree will be massive which you will se when you visualize it.\n", "    Step 3. Fit Classifier to vectorized data and labels\n", "    Step 4. Test the insample error using the score function for the classifier - print the score to screen (use the score function)\n", "    Step 5. Test the error on the test data same was as the trainining data - print the score to screen (use the score function)\n", "    Step 6. Return the trained classifier and the countvectorizer\n", "    \n", "    depending on tree options 90-95% should be possible in accuracy\n", "    \"\"\"    \n", "    c = CountVectorizer()\n", "    classifier = None\n", "    ### YOUR CODE HERE \n", "    ### END CODE\n", "        \n", "    return classifier, c\n", "\n", "keys = [(561010, 'Restauranter'), (620100, 'Computerprogrammering')]\n", "feat_train, feat_test, lab_train, lab_test, cnames = get_branche_data(keys)\n", "classifier, vectorizer = decision_tree_text_classification(feat_train, feat_test, lab_train, lab_test)\n", "names = np.array(vectorizer.get_feature_names())\n", "bag_of_words_feat_test = vectorizer.transform(feat_test)\n", "print_errors(classifier, bag_of_words_feat_test, feat_test, lab_test, [x[1] for x in keys])  \n", "plot_tree(classifier, names)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.0"}}, "nbformat": 4, "nbformat_minor": 2}