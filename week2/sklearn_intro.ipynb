{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Hello World of Machine Learning - Supervised Learning\n",
    "In this exercise we will get some experience with the basic approach to supervised learning using Python and Sklearn\n",
    "\n",
    "In supervised Learning we want to learn/apprxiomate some unknown target function $f$ that maps data to whatever value we are interested in and to do that we are given a data set $D=\\{(x_1,y_1),\\dots,(x_n,y_n))\\}$ where $y_i = f(x_i)$.\n",
    "\n",
    "In these exercises we will focus on classifaction tasks where $f(x_i)\\in \\{0,\\dots,k-1\\}$ for a problem with $k$ output classes.\n",
    "For instance for the problem of Spam Detection, f would be the function that takes as input an email and outputs whehter it is spam or not spam.\n",
    "If we represent an email by a string, then $f$ is a mapping from strings to \\{0, 1\\}, 0 indicating not spam, and 1 indicating spam.\n",
    "To learn this function we are given a list of spam mails (strings) and associated labels (spam, not spam)\n",
    "\n",
    "The very basic supervised learning approach is as follows.\n",
    "* Decide on a (parameterized) set of functions $H$ that we think can be used approximate $f$ on new data. \n",
    "* Use a *learning algorithm* on data $D$ to select a good $h\\in H$ that approximates $f$ well.\n",
    "\n",
    "The measure used by the learning algorithm is defined over the data and designed by the machine learning engineer. It is supposed to encode what a good solution looks like, i.e. what function we prefer over others i.e. a function that minimize some measure on the data, hopefully approximate $f$ well.\n",
    "\n",
    "For classification problems the simplest measure is accuracy, which is the probability that the classifier returns the correct label to a *randomly* selected unknown data point.\n",
    "Measured on a labelled data set is the proportion of data points the algorithm classifies correctly.\n",
    "Given a learned function $h$, and a labelled data set $D=\\{(x_1,y_1),\\dots,(x_n,y_n))\\}$ the accuracy is simple the number of times $h(x_i) = y_i$ divided by n. \n",
    "\n",
    "Enough theory talking. Lets demonstrate with some simple python3 code.\n",
    "\n",
    "# The Classifier Interface\n",
    "The function/learning algorithm $h$ are represented as an object that supports at least the methods **fit, predict, and score**.\n",
    "* fit(X, y), fit the data with learning algorithm on data X with labels y \n",
    "* predict(X), return the prediction of the fitted classifer on data points in X\n",
    "* score(X, y), compute the accuracy of the classifier on data X with labels y\n",
    "\n",
    "In general $X, y$ will be numpy arrays (a numpy array is a multidimensional container). With $n$ data points, $X$ is of shape $(n, d)$ where $d$ is the dimensionality of an input point and $y$ has shape $(n,)$.\n",
    "\n",
    "As a code example see next cell for the implementation very bad classifier. You should be able to figure out what this classifier does. Run the cell to see output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n",
      "The accuracy is 0.5\n"
     ]
    }
   ],
   "source": [
    "# first some magic functions you can ignore\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "class Classifier():\n",
    "    \"\"\" Dummy Classifier Class\"\"\"\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "        @param X, numpy array shape n,d (n input points of dimension d)\n",
    "        @return, numpy array shape n,  vector of predictions y same length as X\n",
    "        \"\"\"\n",
    "        pred = [self.state for x in X]\n",
    "        return np.array(pred)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Trains the classifier\n",
    "        Args:\n",
    "        -----\n",
    "        @param X, numpy array shape n,d (n input points of dimension d)\n",
    "        @param y, numpy array shape n,  vector of labels y same length as X\n",
    "        \"\"\"\n",
    "        self.state = y[0]\n",
    "        return self\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\" Compute the accuracy of classifier on X, y \n",
    "        @param X, numpy array shape n,d (n input points of dimension d)\n",
    "        @param y, numpy array shape n,  vector of labels y same length as X\n",
    "        @return float, accuracy of trained classifier on data X, with labels y\n",
    "        \"\"\"\n",
    "        return np.mean(self.predict(X) == y)\n",
    "    \n",
    "c = Classifier()\n",
    "X = np.array([[2,2], [3,2]])\n",
    "y = np.array([0, 1])\n",
    "c.fit(X, y)\n",
    "pred = c.predict(X)\n",
    "print(pred)\n",
    "print('The accuracy is', c.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a Classifier from the Sklearn Package\n",
    "Let us experiment with the learning models already made by clever people instead of trying to improve the stupid classifier above.\n",
    "Below i have added some code for you to start with.\n",
    "There are two simple utility functions you can ignore, get_2D_data, and visualize2D that generate data, and plot data and classification results resepectively.\n",
    "\n",
    "\n",
    "The job is simply to use class the *Logistic Regression* model from the Sklearn package, which contrary to its name is used for classification not regression.  \n",
    "\n",
    "The gist of it is that an object of the class supports **fit, predict, score** for the machine learning/statistics algorithm known as Logistic Regression.\n",
    "For a long explanation, see http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "\n",
    "Logistic Regression will be covered later in the course.\n",
    "\n",
    "\n",
    "## Exercise\n",
    "Complete the **simple2D_logreg** function. Run the cell so you can see the result and the visualized decision boundary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-51a3365c3160>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[0mfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_2D_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m \u001b[0mlogistic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msimple2D_logreg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[0mvisualize2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogistic\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-51a3365c3160>\u001b[0m in \u001b[0;36msimple2D_logreg\u001b[1;34m(feat, labels)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;31m### YOUR CODE 3 lines\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mlogreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m     \u001b[0mlogreg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;31m### END CODE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def get_2D_data():\n",
    "    \"\"\" Generate a data set to play with\"\"\"\n",
    "    X, y = make_classification(n_samples=500, n_features=2,\n",
    "                               n_redundant=0, n_informative=2,\n",
    "                               random_state=0, n_clusters_per_class=1)\n",
    "    return X, y\n",
    "\n",
    "def visualize2D(classifier, X, y):\n",
    "    \"\"\" Visualize a 2D classifer on data\"\"\"\n",
    "    h = 0.2\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = classifier.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "    ax.contourf(xx, yy, Z, cmap=plt.cm.RdBu)\n",
    "    ax.contour(xx, yy, Z, [0], colors='k', linewidths=3)\n",
    "\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=1-y, cmap=plt.cm.Paired,\n",
    "               edgecolors='black', s=25)\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title('Decision Boundary and Contour Plot - the stronger the color the more certain the prediction. \\\n",
    "                 The black line indicated the data points where the model think the label of the class is 50/50 percent')\n",
    "    plt.show()\n",
    "    \n",
    "def simple2D_logreg(feat, labels):\n",
    "    \"\"\" Run a simple (linear) classification model and see the results\n",
    "\n",
    "    Step 1. Create LogisticRegression classifier object\n",
    "    Step 2. Fit the data using the fit method\n",
    "    Step 3. Print the result (use score function on classfier ) on the training data\n",
    "    Step 4. Return the classifier object\n",
    "    \"\"\"\n",
    "    # knock yourself out - pass means do nothing\n",
    "    ### YOUR CODE 3 lines\n",
    "    ### END CODE\n",
    "\n",
    "feat, labels = get_2D_data()\n",
    "logistic = simple2D_logreg(feat, labels)\n",
    "visualize2D(logistic, feat, labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets try a real data set not in 2D\n",
    "We will take a classic machine learning data set about classifying wine.\n",
    "See https://archive.ics.uci.edu/ml/datasets/wine for info about the data.\n",
    "The data set now has 13 features so we cannot really plot in 2D. \n",
    "We progress nonetheless.\n",
    "\n",
    "As stated in the intro the quest is not to get a classifier that answers correctly on the training data, but on new data not seen by the training algorithm.\n",
    "To measure this out-of-sample accuracy we simply split the data into two. A **training set** and a **test set**.\n",
    "\n",
    "We train the classifier on the training set and test the quality of the result on the test set. This loses data for training (bad!), but at least it gives us an unbiased estimate of the quality of our learned classification algorithm. The test set can really only be used once, but in this exercise we cheat and use it at least three times (functioning as what is known as a validation set not a test set...).\n",
    "\n",
    "We will use three different learning models, the Logistic Regression from above, Decision trees, and the famous Neural Nets (all supporting fit, predict, score).\n",
    "* LogisticRegression:\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.decision_function\n",
    "* DecisionTrees -\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "* Neural Nets -\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "    May be hard to get good results out of the box! Dont lose to much sleep if the results are not to good.\n",
    "    \n",
    "I have put them in three different functions for you to play with.\n",
    "To understand decision trees and what it has learned you can use the visualize tree function that very nicely shows the classifier learned by the decision tree.\n",
    "It is also possible to try and peek at what the other two learning models learn but is not as easily readable.\n",
    "\n",
    "\n",
    "You job is to fill the three training methods and play with some parameters.\n",
    "\n",
    "All learning algorithms have multiple hyperparameters that influence the learning method. Since we have not covered any of them yet,  in this exercises you are only required to try the depth parameter for the trees since that one is fairly easy to understand.\n",
    "\n",
    "See the description in the underlying function.\n",
    "\n",
    "Before you Knock yourself out lets take a short look at the data\n",
    "\n",
    "## The wine data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import pandas as pd\n",
    "from IPython.display import display    \n",
    "data = load_wine()\n",
    "print('Lets See the Features:')\n",
    "\n",
    "for name in data.feature_names:\n",
    "    print('-', name)    \n",
    "print('\\nFrom such data  you must specify one of three wines!!!')\n",
    "print('*'*30)\n",
    "print('Data Size:', data.target.shape[0], 'So Small!')\n",
    "print('Lets see the first 10 data points')\n",
    "print('*'*30)\n",
    "X = data.data\n",
    "y = data.target\n",
    "df = pd.DataFrame(X, columns=data.feature_names)\n",
    "display(df.head(10))\n",
    "print('*'*30)\n",
    "print('lets plot alcohol and magnesium colored with class')\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "ax.scatter(X[:,0], X[:,4], c=y, cmap=plt.cm.Paired, s=20)\n",
    "plt.show()\n",
    "print('Not an obvious pattern - lets learn a classifier instead of fiddling with data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learn to classify wine.\n",
    "I assume that manually looking at this and try to make a rule is not fun, and this data set is in fact very small and simple...\n",
    "So machine learning please help.\n",
    "\n",
    "Your job is to apply machine learning to find a good classifier for wine using the wine data set.\n",
    "## Exercise\n",
    "* Complete train_wine_logistic and uncomment the test lines below\n",
    "* Complete train_wine_dectree and uncomment the test lines below. Try different values for max_depth. The effect should be obvious on the visualized tree.\n",
    "* Complete train_wine_neural_net and uncomment the test lines below. You must set the hidden layer size and changing for instance batch_size as well should help the results. Do not spend all day trying parameters here, but do try a little.\n",
    "\n",
    "Which methods seems best for this data set out of the box? (You can run it multiple time with different results due to randomness of the data split).\n",
    "Can you figure out which features are important for the classification for each model. Could it be important to new the *best* features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Logistic Regression Wine Score ********************\n",
      "In Sample Score:  0.9849624060150376\n",
      "Test Score:  0.9333333333333333\n",
      "Logistic Regreesion Learned Weights: \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "\n",
      " ******************** Decision Tree Wine Score ********************\n",
      "In Sample Score:  1.0\n",
      "Test Score:  0.8888888888888888\n",
      "Lets see the Tree\n"
     ]
    },
    {
     "ename": "InvocationException",
     "evalue": "GraphViz's executables not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvocationException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-a8c468ac70e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[0mprint_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Lets see the Tree'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m \u001b[0mplot_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;31m## try different hidden layer settings, less is probably more\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-a8c468ac70e3>\u001b[0m in \u001b[0;36mplot_tree\u001b[1;34m(dtree, feature_names)\u001b[0m\n\u001b[0;32m     46\u001b[0m                     special_characters=True, feature_names=feature_names)\n\u001b[0;32m     47\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydotplus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_from_dot_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdot_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_png\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m     \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pydotplus\\graphviz.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, prog)\u001b[0m\n\u001b[0;32m   1795\u001b[0m             self.__setattr__(\n\u001b[0;32m   1796\u001b[0m                 \u001b[1;34m'create_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfrmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1797\u001b[1;33m                 \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfrmt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprog\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1798\u001b[0m             )\n\u001b[0;32m   1799\u001b[0m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'create_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfrmt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pydotplus\\graphviz.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, prog, format)\u001b[0m\n\u001b[0;32m   1958\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1959\u001b[0m                 raise InvocationException(\n\u001b[1;32m-> 1960\u001b[1;33m                     'GraphViz\\'s executables not found')\n\u001b[0m\u001b[0;32m   1961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1962\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprog\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvocationException\u001b[0m: GraphViz's executables not found"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image, display\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def bar_plot_weights(weights, names):\n",
    "    \"\"\" Bar plot of weights\"\"\"\n",
    "    features = weights.shape[1]\n",
    "    classes = weights.shape[0]\n",
    "    fig, ax = plt.subplots(figsize=(13, 10))\n",
    "    index = np.arange(features)\n",
    "    bar_width = 0.15\n",
    "    opacity = 0.8\n",
    "    colors = ['r', 'g', 'b']\n",
    "\n",
    "    for i in range(classes):\n",
    "        rects2 = plt.bar(index + i * bar_width, weights[i, :], bar_width,\n",
    "                     alpha=opacity,\n",
    "                     label='Wine {0}'.format(i))\n",
    " \n",
    "    ax.set_xlabel('feature')\n",
    "    ax.set_ylabel('feature weight')\n",
    "    ax.set_title('Wine Feature Weights')\n",
    "    ax.set_xticks(index + bar_width / 3, names)\n",
    "    ax.set_xticklabels(names)\n",
    "    plt.legend()\n",
    " \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_tree(dtree, feature_names):\n",
    "    dot_data = StringIO()\n",
    "    export_graphviz(dtree, out_file=dot_data,\n",
    "                    filled=True, rounded=True,\n",
    "                    special_characters=True, feature_names=feature_names)\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n",
    "    img = Image(graph.create_png())\n",
    "    display(img)\n",
    "\n",
    "\n",
    "def print_score(classifier, X_train, X_test, y_train, y_test):\n",
    "    \"\"\" Simple print score function that prints train and test score of classifier - almost not worth it\"\"\"\n",
    "    print('In Sample Score: ',\n",
    "          classifier.score(X_train, y_train))\n",
    "    print('Test Score: ',\n",
    "          classifier.score(X_test, y_test))\n",
    "\n",
    "    \n",
    "def train_wine_logistic(X_train, y_train):\n",
    "    \"\"\"LogisticRegression as before -\n",
    "        http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.decision_function\n",
    "        return the trained logistic regression object\n",
    "        \n",
    "        For even this simple model there are many choices you can tune, \n",
    "        the most important parameter probably being  the C parameter which controls how \"complex\" we want the function. \n",
    "        The smaller C the simpler function. If you like try different value\n",
    "        \n",
    "        Step 1. Create LogisticRegression classifier object\n",
    "        Step 2. Fit the data using the fit method\n",
    "        Step 3. Return the Classifier\n",
    "    \"\"\"\n",
    "    # Knock your self out\n",
    "    ### YOUR CODE 3 lines\n",
    "    logreg = LogisticRegression()\n",
    "    logreg.fit(X_train, y_train)\n",
    "    return logreg\n",
    "    ### END CODE\n",
    "\n",
    "    \n",
    "def train_wine_dectree(X_train, y_train, max_depth=5):\n",
    "    \"\"\"\n",
    "    DecisionTrees -\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "    \n",
    "    For this DecisionTree model there are even kmore knobs to turn. But for this exercise you must set the max_depth parameter and experiment with it a little. It will greatly affect how intricate a function you can learn.\n",
    "    As talked about shortly, we often prefer simpler explanations if one exists so this is at least one way you can control this.\n",
    "    \n",
    "    The parameter max_depth can be set in the constructor for the tree object (as a named paramater) i.e. t = DecisionTreeClassifier(max_depth=42)\n",
    "    \n",
    "    Remember to return the tree.\n",
    "    \n",
    "        Step 1. Create DecisionTreeClassifier object\n",
    "        Step 2. Fit the data using the fit method\n",
    "        Step 3. Return the classifier \n",
    "    \"\"\"\n",
    "    # knock yourself out\n",
    "    ### YOUR CODE 3 lines\n",
    "    dtree = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    dtree.fit(X_train,y_train)\n",
    "    return dtree\n",
    "    ### END CODE\n",
    "\n",
    "       \n",
    "def train_wine_neural_net(X_train, y_train, hidden_layers=(32, 32, 32)):\n",
    "    \"\"\"\n",
    "   Neural Nets -\n",
    "    http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "    Remember to set the hidden_layer_sizes in the construction i.e. MLPClassifier(hidden_layers_sizes=)\n",
    "    Otherwise train the MLPclassifer and return it/\n",
    "    \n",
    "    May be hard to get good results out of the box! Dont lose to much sleep if the results are not to good.\n",
    "    Setting other hyperparameters away from standard is probably required... \n",
    "    Set batch_size to a small number like 4 or 8 helps significantly\n",
    "    \n",
    "        Step 1. Create MPLClassifer  object\n",
    "        Step 2. Fit the data using the fit method\n",
    "        Step 3. Return the classifier \n",
    "    \"\"\"\n",
    "    # knock yourself out\n",
    "    ### YOUR CODE 3 lines\n",
    "    classifier = MLPClassifier()\n",
    "    classifier.fit(X_train,y_train)\n",
    "    return classifier\n",
    "    ### END CODE\n",
    "\n",
    "# the main method here - comment out stuff you do not want to see\n",
    "## Get Data\n",
    "data = load_wine()\n",
    "X = data.data\n",
    "y = data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "\n",
    "## Try logistic regression\n",
    "head = '*'*20\n",
    "print(head, 'Logistic Regression Wine Score', head)\n",
    "logistic = train_wine_logistic(X_train, y_train)\n",
    "print_score(logistic, X_train, X_test, y_train, y_test)\n",
    "print('Logistic Regreesion Learned Weights: ')\n",
    "bar_plot_weights(logistic.coef_, data.feature_names)\n",
    "print(data.feature_names)\n",
    "\n",
    "## try different max depth\n",
    "print('\\n', head,'Decision Tree Wine Score', head)\n",
    "tree = train_wine_dectree(X_train, y_train, max_depth=5)\n",
    "print_score(tree,  X_train, X_test, y_train, y_test)\n",
    "print('Lets see the Tree')\n",
    "plot_tree(tree, data.feature_names)\n",
    "\n",
    "## try different hidden layer settings, less is probably more\n",
    "print('\\n', head, 'Neural Net Wine Score:', head)\n",
    "mlp = train_wine_neural_net(X_train, y_train, hidden_layers=(10,))\n",
    "print_score(mlp, X_train, X_test, y_train, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
